{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "header-cell",
            "metadata": {},
            "source": [
                "# Multi-Backend Chatbot with Gradio\n",
                "\n",
                "This notebook demonstrates how to build a chatbot that can switch between multiple LLM providers:\n",
                "- **Ollama** (local models)\n",
                "- **OpenRouter** (multiple cloud models)\n",
                "- **GitHub Models** (GitHub's AI models)\n",
                "- **Groq** (fast inference)\n",
                "- **Google AI Studio** (Gemini models)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "imports-cell",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "execution_count": 1,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import gradio as gr\n",
                "import os\n",
                "from dotenv import load_dotenv\n",
                "from openai import OpenAI\n",
                "import google.generativeai\n",
                "\n",
                "# Load environment variables\n",
                "load_dotenv(override=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "9e4a95f2",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Note: you may need to restart the kernel to use updated packages.\n"
                    ]
                }
            ],
            "source": [
                "# Install required packages (run once if needed)\n",
                "%pip install google-generativeai openai python-dotenv gradio -q"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "ollama-func",
            "metadata": {},
            "outputs": [],
            "source": [
                "def chat_with_ollama(message, history, model=\"llama3.2:1b\"):\n",
                "    \"\"\"\n",
                "    Chat with a local Ollama model.\n",
                "    Ensures the Ollama server is running locally at http://localhost:11434/v1\n",
                "    \"\"\"\n",
                "    try:\n",
                "        client = OpenAI(\n",
                "            base_url=\"http://localhost:11434/v1\",\n",
                "            api_key=\"ollama\"  # Required but ignored by Ollama\n",
                "        )\n",
                "        \n",
                "        messages = []\n",
                "        # Construct messages from history\n",
                "        for user_msg, bot_msg in history:\n",
                "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
                "            messages.append({\"role\": \"assistant\", \"content\": bot_msg})\n",
                "        \n",
                "        messages.append({\"role\": \"user\", \"content\": message})\n",
                "\n",
                "        response = client.chat.completions.create(\n",
                "            model=model,\n",
                "            messages=messages,\n",
                "            stream=True\n",
                "        )\n",
                "        \n",
                "        partial_message = \"\"\n",
                "        for chunk in response:\n",
                "            if chunk.choices and chunk.choices[0].delta.content:\n",
                "                partial_message += chunk.choices[0].delta.content\n",
                "                yield partial_message\n",
                "                \n",
                "    except Exception as e:\n",
                "        yield f\"Error connecting to Ollama: {str(e)}. Make sure Ollama is running.\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "openrouter-func",
            "metadata": {},
            "outputs": [],
            "source": [
                "def chat_with_openrouter(message, history, model=\"openai/gpt-3.5-turbo\"):\n",
                "    \"\"\"\n",
                "    Chat with OpenRouter models.\n",
                "    Requires OPENROUTER_API_KEY in .env\n",
                "    \"\"\"\n",
                "    api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
                "    if not api_key:\n",
                "        yield \"Error: OPENROUTER_API_KEY not found in environment variables.\"\n",
                "        return\n",
                "\n",
                "    try:\n",
                "        client = OpenAI(\n",
                "            base_url=\"https://openrouter.ai/api/v1\",\n",
                "            api_key=api_key\n",
                "        )\n",
                "        \n",
                "        messages = []\n",
                "        for user_msg, bot_msg in history:\n",
                "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
                "            messages.append({\"role\": \"assistant\", \"content\": bot_msg})\n",
                "            \n",
                "        messages.append({\"role\": \"user\", \"content\": message})\n",
                "\n",
                "        response = client.chat.completions.create(\n",
                "            model=model,\n",
                "            messages=messages,\n",
                "            stream=True,\n",
                "            extra_headers={\n",
                "                \"HTTP-Referer\": \"http://localhost:7860\",\n",
                "                \"X-Title\": \"Local Chatbot\"\n",
                "            }\n",
                "        )\n",
                "        \n",
                "        partial_message = \"\"\n",
                "        for chunk in response:\n",
                "            if chunk.choices and chunk.choices[0].delta.content:\n",
                "                partial_message += chunk.choices[0].delta.content\n",
                "                yield partial_message\n",
                "                \n",
                "    except Exception as e:\n",
                "        yield f\"Error connecting to OpenRouter: {str(e)}\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "github-func",
            "metadata": {},
            "outputs": [],
            "source": [
                "def chat_with_github(message, history, model=\"gpt-4o-mini\"):\n",
                "    \"\"\"\n",
                "    Chat with GitHub Models.\n",
                "    Requires GITHUB_TOKEN in .env\n",
                "    \"\"\"\n",
                "    api_key = os.getenv(\"GITHUB_TOKEN\")\n",
                "    if not api_key:\n",
                "        yield \"Error: GITHUB_TOKEN not found in environment variables.\"\n",
                "        return\n",
                "\n",
                "    try:\n",
                "        client = OpenAI(\n",
                "            base_url=\"https://models.github.ai/inference\",\n",
                "            api_key=api_key\n",
                "        )\n",
                "        \n",
                "        messages = []\n",
                "        for user_msg, bot_msg in history:\n",
                "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
                "            messages.append({\"role\": \"assistant\", \"content\": bot_msg})\n",
                "            \n",
                "        messages.append({\"role\": \"user\", \"content\": message})\n",
                "\n",
                "        response = client.chat.completions.create(\n",
                "            model=model,\n",
                "            messages=messages,\n",
                "            stream=True\n",
                "        )\n",
                "        \n",
                "        partial_message = \"\"\n",
                "        for chunk in response:\n",
                "            if chunk.choices and chunk.choices[0].delta.content:\n",
                "                partial_message += chunk.choices[0].delta.content\n",
                "                yield partial_message\n",
                "                \n",
                "    except Exception as e:\n",
                "        yield f\"Error connecting to GitHub Models: {str(e)}\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "groq-func",
            "metadata": {},
            "outputs": [],
            "source": [
                "def chat_with_groq(message, history, model=\"llama-3.3-70b-versatile\"):\n",
                "    \"\"\"\n",
                "    Chat with Groq models.\n",
                "    Requires GROQ_API_KEY in .env\n",
                "    \"\"\"\n",
                "    api_key = os.getenv(\"GROQ_API_KEY\")\n",
                "    if not api_key:\n",
                "        yield \"Error: GROQ_API_KEY not found in environment variables.\"\n",
                "        return\n",
                "\n",
                "    try:\n",
                "        client = OpenAI(\n",
                "            base_url=\"https://api.groq.com/openai/v1\",\n",
                "            api_key=api_key\n",
                "        )\n",
                "        \n",
                "        messages = []\n",
                "        for user_msg, bot_msg in history:\n",
                "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
                "            messages.append({\"role\": \"assistant\", \"content\": bot_msg})\n",
                "            \n",
                "        messages.append({\"role\": \"user\", \"content\": message})\n",
                "\n",
                "        response = client.chat.completions.create(\n",
                "            model=model,\n",
                "            messages=messages,\n",
                "            stream=True\n",
                "        )\n",
                "        \n",
                "        partial_message = \"\"\n",
                "        for chunk in response:\n",
                "            if chunk.choices and chunk.choices[0].delta.content:\n",
                "                partial_message += chunk.choices[0].delta.content\n",
                "                yield partial_message\n",
                "                \n",
                "    except Exception as e:\n",
                "        yield f\"Error connecting to Groq: {str(e)}\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "gemini-func",
            "metadata": {},
            "outputs": [],
            "source": [
                "def chat_with_gemini(message, history, model=\"gemini-2.0-flash\"):\n",
                "    \"\"\"\n",
                "    Chat with Google AI Studio (Gemini) models.\n",
                "    Requires GOOGLE_API_KEY in .env\n",
                "    \"\"\"\n",
                "    api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
                "    if not api_key:\n",
                "        yield \"Error: GOOGLE_API_KEY not found in environment variables.\"\n",
                "        return\n",
                "\n",
                "    try:\n",
                "        google.generativeai.configure(api_key=api_key)\n",
                "        gemini = google.generativeai.GenerativeModel(\n",
                "            model_name=model,\n",
                "            system_instruction=\"You are a helpful assistant\"\n",
                "        )\n",
                "        \n",
                "        # Build chat history for Gemini\n",
                "        chat_history = []\n",
                "        for user_msg, bot_msg in history:\n",
                "            chat_history.append({\"role\": \"user\", \"parts\": [user_msg]})\n",
                "            chat_history.append({\"role\": \"model\", \"parts\": [bot_msg]})\n",
                "        \n",
                "        chat = gemini.start_chat(history=chat_history)\n",
                "        response = chat.send_message(message, stream=True)\n",
                "        \n",
                "        partial_message = \"\"\n",
                "        for chunk in response:\n",
                "            partial_message += chunk.text\n",
                "            yield partial_message\n",
                "                \n",
                "    except Exception as e:\n",
                "        yield f\"Error connecting to Gemini: {str(e)}\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "router-func",
            "metadata": {},
            "outputs": [],
            "source": [
                "def chat_router(message, history, backend, ollama_model, openrouter_model, github_model, groq_model, gemini_model):\n",
                "    \"\"\"Route chat requests to the appropriate backend.\"\"\"\n",
                "    if backend == \"Ollama\":\n",
                "        yield from chat_with_ollama(message, history, ollama_model)\n",
                "    elif backend == \"OpenRouter\":\n",
                "        yield from chat_with_openrouter(message, history, openrouter_model)\n",
                "    elif backend == \"GitHub Models\":\n",
                "        yield from chat_with_github(message, history, github_model)\n",
                "    elif backend == \"Groq\":\n",
                "        yield from chat_with_groq(message, history, groq_model)\n",
                "    elif backend == \"Gemini\":\n",
                "        yield from chat_with_gemini(message, history, gemini_model)\n",
                "    else:\n",
                "        yield \"Error: Unknown backend selected.\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "gradio-ui",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "* Running on local URL:  http://127.0.0.1:7860\n",
                        "* To create a public link, set `share=True` in `launch()`.\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# Create Enhanced Gradio Interface\n",
                "\n",
                "# Model options for each backend (Updated December 2025)\n",
                "OLLAMA_MODELS = [\n",
                "    \"llama3.2:1b\", \"llama3.2:3b\", \"llama3.1:8b\", \"llama3.1:70b\",\n",
                "    \"mistral:7b\", \"mixtral:8x7b\", \"codellama:7b\", \"codellama:34b\",\n",
                "    \"phi3:mini\", \"phi3:medium\", \"gemma2:9b\", \"qwen2.5:7b\", \"deepseek-r1:7b\"\n",
                "]\n",
                "\n",
                "# OpenRouter Free Models (Updated December 2025)\n",
                "OPENROUTER_MODELS = [\n",
                "    \"meta-llama/llama-3.3-70b-instruct:free\", \"google/gemini-2.0-flash-exp:free\",\n",
                "    \"tngtech/deepseek-r1t2-chimera:free\", \"tngtech/deepseek-r1t-chimera:free\",\n",
                "    \"tngtech/tng-r1t-chimera:free\", \"allenai/olmo-3-32b-think:free\",\n",
                "    \"alibaba/tongyi-deepresearch-30b-a3b:free\", \"kwaipilot/kat-coder-pro:free\",\n",
                "    \"qwen/qwen3-4b:free\", \"google/gemma-3-27b-it:free\", \"google/gemma-3-12b-it:free\",\n",
                "    \"google/gemma-3-4b-it:free\", \"google/gemma-3n-e2b-it:free\",\n",
                "    \"nvidia/nemotron-nano-12b-v2-vl:free\", \"nvidia/nemotron-nano-9b-v2:free\",\n",
                "    \"mistralai/mistral-small-3.1-24b-instruct:free\", \"mistralai/mistral-7b-instruct:free\",\n",
                "    \"openai/gpt-oss-20b:free\", \"z-ai/glm-4.5-air:free\", \"amazon/nova-2-lite-v1:free\",\n",
                "    \"cognitivecomputations/dolphin-mistral-24b-venice-edition:free\", \"arcee-ai/trinity-mini:free\"\n",
                "]\n",
                "\n",
                "# GitHub Models Marketplace (Free Tier)\n",
                "GITHUB_MODELS = [\n",
                "    \"gpt-5\", \"gpt-5-chat\", \"gpt-5-mini\", \"gpt-5-nano\",\n",
                "    \"o3\", \"o3-mini\", \"o4-mini\", \"gpt-4o\", \"gpt-4o-mini\",\n",
                "    \"Llama-3.3-70B-Instruct\", \"Llama-3.2-90B-Vision-Instruct\",\n",
                "    \"Llama-3.2-11B-Vision-Instruct\", \"Phi-4\", \"Phi-4-reasoning\",\n",
                "    \"Phi-4-multimodal-instruct\", \"Phi-4-mini-reasoning\", \"Phi-4-mini-instruct\",\n",
                "    \"Codestral-2501\", \"Mistral-Small-3-1-multimodal\",\n",
                "    \"DeepSeek-R1\", \"DeepSeek-V3-0324\", \"MAI-DS-R1\", \"AI21-Jamba-1-5-Large\"\n",
                "]\n",
                "\n",
                "# Groq Free Tier Models\n",
                "GROQ_MODELS = [\n",
                "     \"meta-llama/llama-4-maverick-17b-128e-instruct\", \"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
                "    \"llama-3.3-70b-versatile\", \"llama-3.1-8b-instant\",\n",
                "    \"openai/gpt-oss-120b\", \"openai/gpt-oss-20b\", \"openai/gpt-oss-safeguard-20b\",\n",
                "    \"moonshotai/kimi-k2-instruct-0905\", \"qwen/qwen3-32b\",\n",
                "    \"whisper-large-v3\", \"whisper-large-v3-turbo\"\n",
                "]\n",
                "\n",
                "# Google AI Studio / Gemini API Models\n",
                "GEMINI_MODELS = [\n",
                "    \"gemini-3-pro-preview\", \"gemini-3-pro-image-preview\",\n",
                "    \"gemini-2.5-pro\", \"gemini-2.5-flash\", \"gemini-2.5-flash-lite\",\n",
                "    \"gemini-2.0-flash-exp\",\n",
                "    \"gemma-3-27b-it\", \"gemma-3-12b-it\", \"gemma-3-4b-it\", \"gemma-3-1b-it\",\n",
                "    \"gemma-3n-e4b-it\", \"gemma-3n-e2b-it\"\n",
                "]\n",
                "\n",
                "# Custom CSS for enhanced styling\n",
                "custom_css = \"\"\"\n",
                ".gradio-container {\n",
                "    max-width: 1200px !important;\n",
                "}\n",
                ".chat-header {\n",
                "    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
                "    padding: 20px;\n",
                "    border-radius: 10px;\n",
                "    margin-bottom: 20px;\n",
                "}\n",
                ".status-connected {\n",
                "    color: #22c55e;\n",
                "    font-weight: bold;\n",
                "}\n",
                ".status-error {\n",
                "    color: #ef4444;\n",
                "    font-weight: bold;\n",
                "}\n",
                "footer {\n",
                "    visibility: hidden;\n",
                "}\n",
                "\"\"\"\n",
                "\n",
                "def check_api_status():\n",
                "    \"\"\"Check which APIs are configured.\"\"\"\n",
                "    status = []\n",
                "    apis = {\n",
                "        \"Ollama\": (\"Always available (local)\", True),\n",
                "        \"OpenRouter\": (os.getenv(\"OPENROUTER_API_KEY\"), bool(os.getenv(\"OPENROUTER_API_KEY\"))),\n",
                "        \"GitHub Models\": (os.getenv(\"GITHUB_TOKEN\"), bool(os.getenv(\"GITHUB_TOKEN\"))),\n",
                "        \"Groq\": (os.getenv(\"GROQ_API_KEY\"), bool(os.getenv(\"GROQ_API_KEY\"))),\n",
                "        \"Gemini\": (os.getenv(\"GOOGLE_API_KEY\"), bool(os.getenv(\"GOOGLE_API_KEY\")))\n",
                "    }\n",
                "    \n",
                "    for name, (key, available) in apis.items():\n",
                "        icon = \"‚úÖ\" if available else \"‚ùå\"\n",
                "        status.append(f\"{icon} {name}\")\n",
                "    \n",
                "    return \" | \".join(status)\n",
                "\n",
                "def get_model_choices(backend):\n",
                "    \"\"\"Get model choices based on selected backend.\"\"\"\n",
                "    model_map = {\n",
                "        \"Ollama\": OLLAMA_MODELS,\n",
                "        \"OpenRouter\": OPENROUTER_MODELS,\n",
                "        \"GitHub Models\": GITHUB_MODELS,\n",
                "        \"Groq\": GROQ_MODELS,\n",
                "        \"Gemini\": GEMINI_MODELS\n",
                "    }\n",
                "    return gr.update(choices=model_map.get(backend, []), value=model_map.get(backend, [\"\"])[0])\n",
                "\n",
                "def enhanced_chat_router(message, history, backend, model, system_prompt, temperature, max_tokens):\n",
                "    \"\"\"Enhanced chat router with system prompt and temperature support.\"\"\"\n",
                "    \n",
                "    if not message.strip():\n",
                "        yield \"Please enter a message.\"\n",
                "        return\n",
                "    \n",
                "    # Add system prompt handling for backends that support it\n",
                "    if backend == \"Ollama\":\n",
                "        yield from chat_with_ollama_enhanced(message, history, model, system_prompt, temperature, max_tokens)\n",
                "    elif backend == \"OpenRouter\":\n",
                "        yield from chat_with_openrouter_enhanced(message, history, model, system_prompt, temperature, max_tokens)\n",
                "    elif backend == \"GitHub Models\":\n",
                "        yield from chat_with_github_enhanced(message, history, model, system_prompt, temperature, max_tokens)\n",
                "    elif backend == \"Groq\":\n",
                "        yield from chat_with_groq_enhanced(message, history, model, system_prompt, temperature, max_tokens)\n",
                "    elif backend == \"Gemini\":\n",
                "        yield from chat_with_gemini_enhanced(message, history, model, system_prompt, temperature, max_tokens)\n",
                "    else:\n",
                "        yield \"Error: Unknown backend selected.\"\n",
                "\n",
                "def chat_with_ollama_enhanced(message, history, model, system_prompt, temperature, max_tokens):\n",
                "    \"\"\"Enhanced Ollama chat with system prompt and temperature.\"\"\"\n",
                "    try:\n",
                "        client = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
                "        \n",
                "        messages = []\n",
                "        if system_prompt.strip():\n",
                "            messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
                "        \n",
                "        for user_msg, bot_msg in history:\n",
                "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
                "            messages.append({\"role\": \"assistant\", \"content\": bot_msg})\n",
                "        messages.append({\"role\": \"user\", \"content\": message})\n",
                "\n",
                "        response = client.chat.completions.create(\n",
                "            model=model,\n",
                "            messages=messages,\n",
                "            stream=True,\n",
                "            temperature=temperature,\n",
                "            max_tokens=max_tokens if max_tokens > 0 else None\n",
                "        )\n",
                "        \n",
                "        partial_message = \"\"\n",
                "        for chunk in response:\n",
                "            if chunk.choices and chunk.choices[0].delta.content:\n",
                "                partial_message += chunk.choices[0].delta.content\n",
                "                yield partial_message\n",
                "                \n",
                "    except Exception as e:\n",
                "        yield f\"‚ùå **Ollama Error:** {str(e)}\\n\\nüí° Make sure Ollama is running: `ollama serve`\"\n",
                "\n",
                "def chat_with_openrouter_enhanced(message, history, model, system_prompt, temperature, max_tokens):\n",
                "    \"\"\"Enhanced OpenRouter chat with system prompt and temperature.\"\"\"\n",
                "    api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
                "    if not api_key:\n",
                "        yield \"‚ùå **Error:** OPENROUTER_API_KEY not found. Please add it to your .env file.\"\n",
                "        return\n",
                "\n",
                "    try:\n",
                "        client = OpenAI(base_url=\"https://openrouter.ai/api/v1\", api_key=api_key)\n",
                "        \n",
                "        messages = []\n",
                "        if system_prompt.strip():\n",
                "            messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
                "        \n",
                "        for user_msg, bot_msg in history:\n",
                "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
                "            messages.append({\"role\": \"assistant\", \"content\": bot_msg})\n",
                "        messages.append({\"role\": \"user\", \"content\": message})\n",
                "\n",
                "        response = client.chat.completions.create(\n",
                "            model=model,\n",
                "            messages=messages,\n",
                "            stream=True,\n",
                "            temperature=temperature,\n",
                "            max_tokens=max_tokens if max_tokens > 0 else None,\n",
                "            extra_headers={\n",
                "                \"HTTP-Referer\": \"http://localhost:7860\",\n",
                "                \"X-Title\": \"Universal Chatbot\"\n",
                "            }\n",
                "        )\n",
                "        \n",
                "        partial_message = \"\"\n",
                "        for chunk in response:\n",
                "            if chunk.choices and chunk.choices[0].delta.content:\n",
                "                partial_message += chunk.choices[0].delta.content\n",
                "                yield partial_message\n",
                "                \n",
                "    except Exception as e:\n",
                "        yield f\"‚ùå **OpenRouter Error:** {str(e)}\"\n",
                "\n",
                "def chat_with_github_enhanced(message, history, model, system_prompt, temperature, max_tokens):\n",
                "    \"\"\"Enhanced GitHub Models chat with system prompt and temperature.\"\"\"\n",
                "    api_key = os.getenv(\"GITHUB_TOKEN\")\n",
                "    if not api_key:\n",
                "        yield \"‚ùå **Error:** GITHUB_TOKEN not found. Please add it to your .env file.\"\n",
                "        return\n",
                "\n",
                "    try:\n",
                "        client = OpenAI(base_url=\"https://models.github.ai/inference\", api_key=api_key)\n",
                "        \n",
                "        messages = []\n",
                "        if system_prompt.strip():\n",
                "            messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
                "        \n",
                "        for user_msg, bot_msg in history:\n",
                "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
                "            messages.append({\"role\": \"assistant\", \"content\": bot_msg})\n",
                "        messages.append({\"role\": \"user\", \"content\": message})\n",
                "\n",
                "        response = client.chat.completions.create(\n",
                "            model=model,\n",
                "            messages=messages,\n",
                "            stream=True,\n",
                "            temperature=temperature,\n",
                "            max_tokens=max_tokens if max_tokens > 0 else None\n",
                "        )\n",
                "        \n",
                "        partial_message = \"\"\n",
                "        for chunk in response:\n",
                "            if chunk.choices and chunk.choices[0].delta.content:\n",
                "                partial_message += chunk.choices[0].delta.content\n",
                "                yield partial_message\n",
                "                \n",
                "    except Exception as e:\n",
                "        yield f\"‚ùå **GitHub Models Error:** {str(e)}\"\n",
                "\n",
                "def chat_with_groq_enhanced(message, history, model, system_prompt, temperature, max_tokens):\n",
                "    \"\"\"Enhanced Groq chat with system prompt and temperature.\"\"\"\n",
                "    api_key = os.getenv(\"GROQ_API_KEY\")\n",
                "    if not api_key:\n",
                "        yield \"‚ùå **Error:** GROQ_API_KEY not found. Please add it to your .env file.\"\n",
                "        return\n",
                "\n",
                "    try:\n",
                "        client = OpenAI(base_url=\"https://api.groq.com/openai/v1\", api_key=api_key)\n",
                "        \n",
                "        messages = []\n",
                "        if system_prompt.strip():\n",
                "            messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
                "        \n",
                "        for user_msg, bot_msg in history:\n",
                "            messages.append({\"role\": \"user\", \"content\": user_msg})\n",
                "            messages.append({\"role\": \"assistant\", \"content\": bot_msg})\n",
                "        messages.append({\"role\": \"user\", \"content\": message})\n",
                "\n",
                "        response = client.chat.completions.create(\n",
                "            model=model,\n",
                "            messages=messages,\n",
                "            stream=True,\n",
                "            temperature=temperature,\n",
                "            max_tokens=max_tokens if max_tokens > 0 else None\n",
                "        )\n",
                "        \n",
                "        partial_message = \"\"\n",
                "        for chunk in response:\n",
                "            if chunk.choices and chunk.choices[0].delta.content:\n",
                "                partial_message += chunk.choices[0].delta.content\n",
                "                yield partial_message\n",
                "                \n",
                "    except Exception as e:\n",
                "        yield f\"‚ùå **Groq Error:** {str(e)}\"\n",
                "\n",
                "def chat_with_gemini_enhanced(message, history, model, system_prompt, temperature, max_tokens):\n",
                "    \"\"\"Enhanced Gemini chat with system prompt and temperature.\"\"\"\n",
                "    api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
                "    if not api_key:\n",
                "        yield \"‚ùå **Error:** GOOGLE_API_KEY not found. Please add it to your .env file.\"\n",
                "        return\n",
                "\n",
                "    try:\n",
                "        google.generativeai.configure(api_key=api_key)\n",
                "        \n",
                "        generation_config = {\n",
                "            \"temperature\": temperature,\n",
                "        }\n",
                "        if max_tokens > 0:\n",
                "            generation_config[\"max_output_tokens\"] = max_tokens\n",
                "        \n",
                "        gemini = google.generativeai.GenerativeModel(\n",
                "            model_name=model,\n",
                "            system_instruction=system_prompt if system_prompt.strip() else \"You are a helpful assistant\",\n",
                "            generation_config=generation_config\n",
                "        )\n",
                "        \n",
                "        chat_history = []\n",
                "        for user_msg, bot_msg in history:\n",
                "            chat_history.append({\"role\": \"user\", \"parts\": [user_msg]})\n",
                "            chat_history.append({\"role\": \"model\", \"parts\": [bot_msg]})\n",
                "        \n",
                "        chat = gemini.start_chat(history=chat_history)\n",
                "        response = chat.send_message(message, stream=True)\n",
                "        \n",
                "        partial_message = \"\"\n",
                "        for chunk in response:\n",
                "            partial_message += chunk.text\n",
                "            yield partial_message\n",
                "                \n",
                "    except Exception as e:\n",
                "        yield f\"‚ùå **Gemini Error:** {str(e)}\"\n",
                "\n",
                "def export_chat(history):\n",
                "    \"\"\"Export chat history to markdown format.\"\"\"\n",
                "    if not history:\n",
                "        return \"No chat history to export.\"\n",
                "    \n",
                "    markdown = \"# Chat Export\\n\\n\"\n",
                "    for i, (user_msg, bot_msg) in enumerate(history, 1):\n",
                "        markdown += f\"## Turn {i}\\n\\n\"\n",
                "        markdown += f\"**User:** {user_msg}\\n\\n\"\n",
                "        markdown += f\"**Assistant:** {bot_msg}\\n\\n\"\n",
                "        markdown += \"---\\n\\n\"\n",
                "    \n",
                "    return markdown\n",
                "\n",
                "# Build the enhanced interface\n",
                "with gr.Blocks(css=custom_css, theme=gr.themes.Soft(primary_hue=\"indigo\", secondary_hue=\"purple\")) as demo:\n",
                "    \n",
                "    # Header\n",
                "    gr.Markdown(\"\"\"\n",
                "    # ü§ñ Universal AI Chatbot\n",
                "    ### Connect to multiple AI providers with a unified interface\n",
                "    \"\"\")\n",
                "    \n",
                "    # API Status\n",
                "    with gr.Row():\n",
                "        api_status = gr.Markdown(f\"**API Status:** {check_api_status()}\")\n",
                "        refresh_btn = gr.Button(\"üîÑ Refresh Status\", size=\"sm\", scale=0)\n",
                "    \n",
                "    refresh_btn.click(fn=lambda: f\"**API Status:** {check_api_status()}\", outputs=api_status)\n",
                "    \n",
                "    with gr.Row():\n",
                "        # Left sidebar for settings\n",
                "        with gr.Column(scale=1, min_width=300):\n",
                "            gr.Markdown(\"### ‚öôÔ∏è Settings\")\n",
                "            \n",
                "            backend_dropdown = gr.Dropdown(\n",
                "                choices=[\"Ollama\", \"OpenRouter\", \"GitHub Models\", \"Groq\", \"Gemini\"],\n",
                "                value=\"Ollama\",\n",
                "                label=\"üîå Backend Provider\",\n",
                "                info=\"Select your AI provider\"\n",
                "            )\n",
                "            \n",
                "            model_dropdown = gr.Dropdown(\n",
                "                choices=OLLAMA_MODELS,\n",
                "                value=OLLAMA_MODELS[0],\n",
                "                label=\"üß† Model\",\n",
                "                info=\"Select the model to use\",\n",
                "                allow_custom_value=True\n",
                "            )\n",
                "            \n",
                "            with gr.Accordion(\"üéõÔ∏è Advanced Settings\", open=False):\n",
                "                system_prompt = gr.Textbox(\n",
                "                    value=\"You are a helpful, friendly, and knowledgeable AI assistant.\",\n",
                "                    label=\"System Prompt\",\n",
                "                    lines=3,\n",
                "                    placeholder=\"Enter a system prompt to set the AI's behavior...\"\n",
                "                )\n",
                "                \n",
                "                temperature = gr.Slider(\n",
                "                    minimum=0.0,\n",
                "                    maximum=2.0,\n",
                "                    value=0.7,\n",
                "                    step=0.1,\n",
                "                    label=\"üå°Ô∏è Temperature\",\n",
                "                    info=\"Higher = more creative, Lower = more focused\"\n",
                "                )\n",
                "                \n",
                "                max_tokens = gr.Slider(\n",
                "                    minimum=0,\n",
                "                    maximum=4096,\n",
                "                    value=0,\n",
                "                    step=128,\n",
                "                    label=\"üìè Max Tokens\",\n",
                "                    info=\"0 = no limit\"\n",
                "                )\n",
                "            \n",
                "            # Preset system prompts\n",
                "            with gr.Accordion(\"üìù Preset Prompts\", open=False):\n",
                "                preset_prompts = gr.Radio(\n",
                "                    choices=[\n",
                "                        \"Default Assistant\",\n",
                "                        \"Code Expert\",\n",
                "                        \"Creative Writer\",\n",
                "                        \"Data Analyst\",\n",
                "                        \"Teacher\"\n",
                "                    ],\n",
                "                    label=\"Quick Presets\",\n",
                "                    value=\"Default Assistant\"\n",
                "                )\n",
                "            \n",
                "            # Export functionality\n",
                "            with gr.Accordion(\"üíæ Export Chat\", open=False):\n",
                "                export_btn = gr.Button(\"üì• Export to Markdown\", variant=\"secondary\")\n",
                "                export_output = gr.Textbox(label=\"Exported Chat\", lines=5, visible=False)\n",
                "        \n",
                "        # Main chat area\n",
                "        with gr.Column(scale=3):\n",
                "            chatbot_display = gr.Chatbot(\n",
                "                label=\"Chat\",\n",
                "                height=500,\n",
                "                show_copy_button=True,\n",
                "                avatar_images=(None, \"https://api.dicebear.com/7.x/bottts/svg?seed=ai\"),\n",
                "                type=\"messages\"\n",
                "            )\n",
                "            \n",
                "            with gr.Row():\n",
                "                msg_input = gr.Textbox(\n",
                "                    placeholder=\"Type your message here... (Press Enter to send)\",\n",
                "                    label=\"Message\",\n",
                "                    scale=4,\n",
                "                    show_label=False\n",
                "                )\n",
                "                send_btn = gr.Button(\"üì§ Send\", variant=\"primary\", scale=1)\n",
                "            \n",
                "            with gr.Row():\n",
                "                clear_btn = gr.Button(\"üóëÔ∏è Clear Chat\", variant=\"secondary\")\n",
                "                stop_btn = gr.Button(\"‚èπÔ∏è Stop\", variant=\"stop\")\n",
                "    \n",
                "    # Footer\n",
                "    gr.Markdown(\"\"\"\n",
                "    ---\n",
                "    <center>\n",
                "    \n",
                "    üí° **Tips:** Use the sidebar to switch providers, adjust temperature for creativity, and set custom system prompts.\n",
                "    \n",
                "    </center>\n",
                "    \"\"\")\n",
                "    \n",
                "    # Event handlers\n",
                "    backend_dropdown.change(\n",
                "        fn=get_model_choices,\n",
                "        inputs=backend_dropdown,\n",
                "        outputs=model_dropdown\n",
                "    )\n",
                "    \n",
                "    # Preset prompt handler\n",
                "    def set_preset_prompt(preset):\n",
                "        presets = {\n",
                "            \"Default Assistant\": \"You are a helpful, friendly, and knowledgeable AI assistant.\",\n",
                "            \"Code Expert\": \"You are an expert programmer. Provide clean, efficient, well-documented code with clear explanations. Always consider best practices and edge cases.\",\n",
                "            \"Creative Writer\": \"You are a creative writer with a flair for engaging storytelling. Use vivid language, creative metaphors, and compelling narratives.\",\n",
                "            \"Data Analyst\": \"You are a data analysis expert. Provide insights, explain statistical concepts clearly, and suggest appropriate visualization and analysis methods.\",\n",
                "            \"Teacher\": \"You are a patient and encouraging teacher. Explain concepts step by step, use examples, and check for understanding.\"\n",
                "        }\n",
                "        return presets.get(preset, presets[\"Default Assistant\"])\n",
                "    \n",
                "    preset_prompts.change(\n",
                "        fn=set_preset_prompt,\n",
                "        inputs=preset_prompts,\n",
                "        outputs=system_prompt\n",
                "    )\n",
                "    \n",
                "    # Chat function\n",
                "    def respond(message, history, backend, model, system_prompt, temperature, max_tokens):\n",
                "        if not message.strip():\n",
                "            return \"\", history\n",
                "        \n",
                "        # Convert messages format history to tuples for the chat functions\n",
                "        tuple_history = []\n",
                "        for msg in history:\n",
                "            if isinstance(msg, dict):\n",
                "                if msg.get(\"role\") == \"user\":\n",
                "                    tuple_history.append([msg.get(\"content\", \"\"), \"\"])\n",
                "                elif msg.get(\"role\") == \"assistant\" and tuple_history:\n",
                "                    tuple_history[-1][1] = msg.get(\"content\", \"\")\n",
                "        \n",
                "        # Add user message\n",
                "        history = history + [{\"role\": \"user\", \"content\": message}]\n",
                "        \n",
                "        for response in enhanced_chat_router(message, tuple_history, backend, model, system_prompt, temperature, max_tokens):\n",
                "            # Add/update assistant response\n",
                "            if history and history[-1].get(\"role\") == \"assistant\":\n",
                "                history[-1][\"content\"] = response\n",
                "            else:\n",
                "                history = history + [{\"role\": \"assistant\", \"content\": response}]\n",
                "            yield \"\", history\n",
                "    \n",
                "    # Submit handlers\n",
                "    submit_event = msg_input.submit(\n",
                "        fn=respond,\n",
                "        inputs=[msg_input, chatbot_display, backend_dropdown, model_dropdown, system_prompt, temperature, max_tokens],\n",
                "        outputs=[msg_input, chatbot_display]\n",
                "    )\n",
                "    \n",
                "    click_event = send_btn.click(\n",
                "        fn=respond,\n",
                "        inputs=[msg_input, chatbot_display, backend_dropdown, model_dropdown, system_prompt, temperature, max_tokens],\n",
                "        outputs=[msg_input, chatbot_display]\n",
                "    )\n",
                "    \n",
                "    # Stop button\n",
                "    stop_btn.click(fn=None, cancels=[submit_event, click_event])\n",
                "    \n",
                "    # Clear chat\n",
                "    clear_btn.click(fn=lambda: ([], \"\"), outputs=[chatbot_display, msg_input])\n",
                "    \n",
                "    # Export chat\n",
                "    def do_export(history):\n",
                "        # Convert messages format to text\n",
                "        if not history:\n",
                "            return gr.update(visible=True, value=\"No chat history to export.\")\n",
                "        \n",
                "        markdown = \"# Chat Export\\n\\n\"\n",
                "        turn = 0\n",
                "        for msg in history:\n",
                "            if isinstance(msg, dict):\n",
                "                role = msg.get(\"role\", \"unknown\")\n",
                "                content = msg.get(\"content\", \"\")\n",
                "                if role == \"user\":\n",
                "                    turn += 1\n",
                "                    markdown += f\"## Turn {turn}\\n\\n\"\n",
                "                    markdown += f\"**User:** {content}\\n\\n\"\n",
                "                elif role == \"assistant\":\n",
                "                    markdown += f\"**Assistant:** {content}\\n\\n---\\n\\n\"\n",
                "        \n",
                "        return gr.update(visible=True, value=markdown)\n",
                "    \n",
                "    export_btn.click(fn=do_export, inputs=chatbot_display, outputs=export_output)\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    demo.launch(share=False)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "llms",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
